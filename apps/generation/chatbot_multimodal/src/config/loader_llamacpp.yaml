cpu: false            # Use the CPU to generate text
cache_type: fp16      # KV cache type; valid options: llama.cpp - fp16, q8_0, q4_0 / ExLlamaV2 - fp16, fp8, q8, q6, q4.
cfg_cache: false      # Create an additional cache for CFG negative prompts. Necessary to use CFG with that loader

flash_attn: false
tensorcores: false    # NVIDIA only: use llama-cpp-python compiled without GGML_CUDA_FORCE_MMQ
cache_capacity: 1Gib  # Max cache capacity. Examples: 2000MiB, 2GiB. If no unit, bytes is assumed.
n_ctx: 8192       # Size of the prompt context
n_batch: 512      # Max number of prompt tokens to batch together when calling llama_eval
n_threads: 0        # Number of threads to use
n_threads_batch: 0  # Number of threads to use for batches / prompt processing
mul_mat_q: true       # Enable the mulmat kernels
use_mmap: true        # Enable memory-map
use_mlock: false      # Force the system to keep the model in RAM
numa: false       # Activate NUMA task allocation for llama.cpp
n_gpu_layers: 0   # Number of layers to offload to the GPU
tensor_split: null    # Split model across GPUs. Comma-separated list of proportions. Example: 60,40
split_mode: 1         # 1: layer-wise / 2: row-wise. For more details: https://dottxt-ai.github.io/outlines/latest/reference/models/llamacpp/
logits_all: false     # Needs to be set for `perplexity` evaluation to work
offload_kqv: true     # Do not offload K, Q, V to GPU. This saves VRAM but reduces the performance
streaming_llm: false      # Activate StreamingLLM to avoid re-evaluating entire prompt when old messages are removed
attention_sink_size: 5    # StreamingLLM: number of sink tokens. Only used if trimmed prompt does not share a prefix with old prompt
tokenizer_dir: "./"       # Load the tokenizer from this folder. Meant to be used with llamacpp_HF through CMD

# Transformers & Accelerate
use_fast_tokenizer: false     # Whether to `use_fast` while loading the tokenizer
trust_remote_code: true       # To allow customized code execution.

# RoPE
# rope_freq_base = 10000 * alpha_value ^ (64 / 63)
# Use either `alpha_value` or `compress_pos_emb`, not both.
rope_freq_base: 1   # If greater than 0, it be used instead of alpha_value.
alpha_value: 1.0    # Positional embeddings alpha factor for NTK RoPE scaling.
compress_pos_emb: 1 # Positional embeddings compression factor. Equal to 1 / rope_freq_scale
