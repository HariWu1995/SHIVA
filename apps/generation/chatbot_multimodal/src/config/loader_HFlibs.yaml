# Transformers & Accelerate
cpu: false            # Use the CPU to generate text
auto_devices: false   # Automatically split the model across the available GPU(s) and CPU.
gpu_memory:   # Max GPU memory in GB to be allocated per GPU (or --gpu-memory 3500MiB for MB)
  - 5         # Example: --gpu-memory 10 for a 1 GPU, --gpu-memory 10 5 for 2 GPUs.
cpu_memory:   # Max CPU memory in GB to allocate for offloaded weights
  - 10        # Example: --cpu-memory 10 for GB or --cpu-memory 3500MiB for MB
disk: true                  # If model is too large for all GPU(s) and CPU combined, send the remaining layers to disk.
disk_cache_dir: "./cache"   # Directory to save the disk cache to
no_cache: false       # Don't use cache in generation. This reduces VRAM usage slightly with a performance cost.
trust_remote_code: true       # To allow customized code execution.
use_safetensors: false        # To prevent arbitrary code execution.
use_bf16: false               # BF16 is better suited for large-scale model training, while FP16 is preferred for tasks requiring high precision. 
use_fast_tokenizer: false     # Whether to `use_fast` while loading the tokenizer
use_flash_attention_2: false  # Whether to `use_flash_attention_2` while loading the model
attn_implementation: "eager"  # Use `attn_implementation` while loading the model
torch_compile: false      # Compile the model with torch.compile for improved performance
streaming_llm: false      # Activate StreamingLLM to avoid re-evaluating entire prompt when old messages are removed

# bitsandbytes
load_in_8bit: false       # Load the model with 8-bit precision
load_in_4bit: false
use_double_quant: false   # use_double_quant for 4-bit
quant_type: nf4           #       quant_type for 4-bit. (nf4, fp4)
compute_dtype: float16    #    compute dtype for 4-bit. (bfloat16, float16, float32)

# DeepSpeed
use_deepspeed: false      # whether to use DeepSpeed ZeRO-3 for inference via Transformers integration
nvme_offload_dir: "./"    # Directory to use for ZeRO-3 NVME offloading
local_rank: 0             # for distributed setup

# RoPE
# rope_freq_base = 10000 * alpha_value ^ (64 / 63)
# Use either `alpha_value` or `compress_pos_emb`, not both.
rope_freq_base: 1   # If greater than 0, it be used instead of alpha_value.
alpha_value: 1.0    # Positional embeddings alpha factor for NTK RoPE scaling.
compress_pos_emb: 1 # Positional embeddings compression factor. Equal to 1 / rope_freq_scale
