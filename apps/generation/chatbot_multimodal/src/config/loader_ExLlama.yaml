cache_type: fp16    # KV cache type; valid options: llama.cpp - fp16, q8_0, q4_0 / ExLlamaV2 - fp16, fp8, q8, q6, q4.
cfg_cache: false    # Create an additional cache for CFG negative prompts. Necessary to use CFG with that loader

gpu_split: 5        # Comma-separated list of VRAM (in GB) to use per GPU device for model layers. Example: 20,7,7.
autosplit: false    # Autosplit the model tensors across available GPUs. When trigger, `gpu_split` is ignored
max_seq_len: 8192   # Maximum sequence length
num_experts_per_token: 2    # Number of experts to use for generation. Applies to MoE models like Mixtral.
use_fast_tokenizer: false
trust_remote_code: false
enable_parallel: false  # Enable Tensor Parallelism (TP)
no_flash_attn: true
no_xformers: true
no_sdpa: true

# RoPE
# rope_freq_base = 10000 * alpha_value ^ (64 / 63)
# Use either `alpha_value` or `compress_pos_emb`, not both.
rope_freq_base: 1   # If greater than 0, it be used instead of alpha_value.
alpha_value: 1.0    # Positional embeddings alpha factor for NTK RoPE scaling.
compress_pos_emb: 1 # Positional embeddings compression factor. Equal to 1 / rope_freq_scale
