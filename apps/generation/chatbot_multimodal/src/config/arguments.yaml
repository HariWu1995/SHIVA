# Basic
character: Assistant  # The name of the character to load in chat mode by default.
verbose: true         # Print the prompts to the terminal
idle_timeout: -1      # Unload model after this minutes of inactivity. It will be automatically reloaded if needed.

# Generation
cpu: false            # Use the CPU to generate text
auto_devices: false   # Automatically split the model across the available GPU(s) and CPU.
gpu_memory:   # Max GPU memory in GB to be allocated per GPU (or --gpu-memory 3500MiB for MB)
  - 5         # Example: --gpu-memory 10 for a 1 GPU, --gpu-memory 10 5 for 2 GPUs.
cpu_memory: 10    # Max CPU memory in GB to allocate for offloaded weights
                  # Example: --cpu-memory 10 for GB or --cpu-memory 3500MiB for MB
disk: true                  # If model is too large for all GPU(s) and CPU combined, send the remaining layers to disk.
disk_cache_dir: "./cache"   # Directory to save the disk cache to
no_cache: false       # Don't use cache in generation. This reduces VRAM usage slightly with a performance cost.
torch_compile: false      # Compile the model with torch.compile for improved performance
streaming_llm: false      # Activate StreamingLLM to avoid re-evaluating entire prompt when old messages are removed
max_seq_len: 8192
n_ctx: 8192

# Cache
cache_type: fp16      # KV cache type; valid options: llama.cpp - fp16, q8_0, q4_0 / ExLlamaV2 - fp16, fp8, q8, q6, q4.
low_cpu_mem: true     # Low CPU memory usage
disk_cache_dir: "./cache"   # Directory to save the disk cache to

# Multimodal
multimodal_pipeline: Null   # The multimodal pipeline to use. Examples: llava-7b, llava-13b.

# Deprecated
cache_4bit: false
cache_8bit: false
triton: false
inject_fused_mlp: true
use_cuda_fp16: true
disable_exllama: false
disable_exllamav2: false
wbits: 0
groupsize: -1
